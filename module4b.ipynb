{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83f06fac633dbf2b3c667850b435f8b9",
     "grade": false,
     "grade_id": "cell-49ffcca1736c96df",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Module 4b: TF-IDF vectorization\n",
    "\n",
    "Een bron van veel informatie over de inhoud van films is de ondertiteling. In deze submodule gaan we kijken hoe we hier gebruik van kunnen maken. Via websites zoals [deze](https://www.opensubtitles.org/) kan je de ondertitels van films downloaden. Om tijd te besparen (en omdat 7000+ ondertitels downloaden niet zo'n leerzame bezigheid is) hebben wij dit vast gedaan.\n",
    "\n",
    "Om iets nuttigs te kunnen doen met deze ondertitels zal je de ingesloten tekstuele informatie op de een of ander manier moeten structureren. Het structureren van natuurlijke tekst heet Natural Language Processing (NLP). NLP is een heel vakgebied op zichzelf waar we ons nu slechts deels in gaan verdiepen. Gelukkig is er spaCy, een Python package die NLP toegankelijk maakt voor onderzoekers die vooral bezig zijn met het verwerken van data. Dit gebeurt door veel keuzes voor aanpakken/algoritmes al voor de gebruiker te maken.\n",
    "\n",
    "SpaCy komt met een grote verzameling aan nuttige tools voor het analyseren van tekst. Wij zullen het in deze submodule vooral gebruiken voor het _tokenizen_ (opsplitsen in losse woorden) van tekst.\n",
    "\n",
    "We hebben ondertitelfiles bestaande uit vele regels tekst. In deze teksten staat nog een hoop rommel die voor ons niet interessant is; hoofdletters, leestekens, html tags, enters, etc. Het opdelen van de tekst in losse woorden heet _tokenization_. Dit is een fundamentele stap in bijna alle NLP taken. Nu hebben jullie al kennis gemaakt met Reguliere Expressies en zouden jullie dit in theorie daarmee kunnen afhandelen, maar met spaCy gaat dit allemaal een stuk eenvoudiger.\n",
    "\n",
    "## Installatie\n",
    "\n",
    "SpaCy is al een onderdeel van de conda CI enviromnet die je aan het begin van dit vak hebt geïnstalleerd. Dus dat hoef je niet meer te installeren. Maar spaCy werkt met taalmodellen die je los moet downloaden. Dat moet je nog wel doen. Voor het downloaden van de taalmodellen van het Engels, typ de volgende commando's in je terminal:\n",
    "\n",
    "    python -m spacy download en_core_web_sm\n",
    "    python -m spacy download en_core_web_md\n",
    "\n",
    "\n",
    "## Oefeningen\n",
    "\n",
    "Doorloop het eerste hoofdstuk van de spaCy-cursus: [spaCy: Finding words, phrases, names and concepts](https://course.spacy.io/chapter1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d9251a74a98207c09e2b634631bf904",
     "grade": false,
     "grade_id": "cell-a932fcc9bd41aa3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Achtergrond\n",
    "\n",
    "Veel vormen van content-based filtering werken uiteindelijk toe naar het maken van een similarity matrix voor items. Zoals we bij collaborative filtering (en content-based filtering aan de hand van genres) hebben gezien, zodra je weet in welke mate items (films) op elkaar lijken, kan je aan de hand daarvan aanbevelingen doen. \n",
    "\n",
    "In alle gevallen die we tot nu toe hebben gezien maakten we een utility matrix. In deze matrix komt elke film overeen met een serie getallen. In het geval van collaborative filtering waren dit de ratings. In het geval van content-based filtering aan de hand van genres, waren dit de eentjes een nulletjes die aangaven of een film een bepaald genre had of niet.\n",
    "\n",
    "Het assicieren van een item met een reeks getallen wordt *vectorisatie* genoemd. (Zo'n reeks getallen wordt ook wel een vector genoemd.) We hebben dus twee vormen van vectorisatie van films gezien: Bij collaborative filtering is elke film een vector met ratings. Bij content-based filtering is elke film een vector met eentjes en nulletjes (gegeven een genre).\n",
    "\n",
    "Door de data op deze manier te vectoriseren kunnen we similatity maten zoals de cosine similarity en de jaccard index om een similarity matrix te maken.\n",
    "\n",
    "Ook als we films met elkaar willen vergelijken aan de hand van de ondertiteling willen we uiteindelijk zo'n vectorisatie maken. Dus we willen de ondertiteling omzetten in een reeks getallen die we met elkaar kunnen vergelijken. Dat kan op meerdere manieren. In deze submodule gebruiken we daar *TF-IDF vectorisatie* voor. In een volgende module ga je nog naar een andere manier kijken.\n",
    "\n",
    "TF-IDF staat voor *term frequency - inverse document frequency*. Het idee is dat we voor elk woord in een tekst een score willen geven. De score geeft aan hoe representatief de score voor de tekst is. Met ander woorden een hoge score betekent dat het gegeven woord veel vaker in de tekst voorkomt dan in andere teksten. Hoe je die score precies bepaald kom je in deze opdracht achter.\n",
    "\n",
    "Door voor elk woord zo'n score te genereren kunnen we teksten met elkaar vergelijken aan de hand van de betreffende scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00bbdd5d16559eb47cc25016d45fa1e8",
     "grade": false,
     "grade_id": "cell-b812d9ed3668c79a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Opdracht\n",
    "\n",
    "Begin met het laden van de benodige libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f59c2abf2c0f16b3d80e29626b4cfb80",
     "grade": false,
     "grade_id": "cell-c6fb025ccf0c528b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import answers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3fedb3daad0f026125c37eb7617f09a",
     "grade": false,
     "grade_id": "cell-13cea20b4f2b9d81",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data laden\n",
    "\n",
    "We laden eerst tien willekeurige ondertitelingen om mee te experimenteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e415455a1878832a1741d950673d50db",
     "grade": false,
     "grade_id": "cell-8f60dfdea771363a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "txt_path = \"./data/txts\"\n",
    "\n",
    "# load 10 arbitrary files\n",
    "files = os.listdir(txt_path)[120:130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f4fe6fea82d12c16f5f30cb989a5695",
     "grade": false,
     "grade_id": "cell-7391acd346c5902e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "De data bestaat nu uit grote lappen tekst. Het eerste wat we moeten doen is de tekst opbreken in losse woorden (tokenizing). Hieronder tokenizen we de ondertitelingen en slaan we het resultaat op in de Series `movie_subs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd74fda6197d5b6d4dcaa93de4cfc03d",
     "grade": false,
     "grade_id": "cell-18dde0aa4405965f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Laad het taalmodel voor Engels\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "movie_ids = []\n",
    "movie_subs = []\n",
    "\n",
    "# Tokenize ieder van de losse ondertitelfiles\n",
    "for ix, file in enumerate(files):\n",
    "    with open(f'{txt_path}/{file}', 'r', encoding='iso-8859-1') as sub_file:\n",
    "        # Haal de movieID uit de filenaam\n",
    "        movie_ids.append(int(file.split('_')[0]))\n",
    "        \n",
    "        # Haal alle HTML tags zoals <i></i> weg.\n",
    "        content = sub_file.read()\n",
    "        content = re.sub(r\"<[^>]*>\", \"\", content)\n",
    "        \n",
    "        # Lees, verwerk, sla op...\n",
    "        doc = nlp(content)\n",
    "        movie_subs.append(doc)\n",
    "        \n",
    "# ... en maak de series\n",
    "movie_subs = pd.Series(index=movie_ids, data=movie_subs)\n",
    "display(movie_subs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8f37d8171a04865ccbd156ad16fcac1",
     "grade": false,
     "grade_id": "cell-2bfc496d7d706ca3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Als je wilt weten welke films dit precies zijn kan je de indices terugvinden in het `movies.csv` bestand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e84b55ad73e2469ae8ff11982d20b5b7",
     "grade": false,
     "grade_id": "cell-309089c4cca5e67b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "datapath = \"./data/ml-latest-small\"\n",
    "\n",
    "# df_movies.loc[193581]\n",
    "df_movies = pd.read_csv(f\"{datapath}/movies.csv\", index_col=0)\n",
    "display(df_movies.loc[movie_subs.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7db36978fb37850db1837cd37908738d",
     "grade": false,
     "grade_id": "cell-a6c853f9da441e99",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Bag of Words\n",
    "\n",
    "Nu we alle ondertitelingen hebben getokenized met spaCy kunnen we bij alle onderdelen van de tekst. Nu weten we ook met redelijke zekerheid wat de werkwoorden zijn, wat de lidwoorden of wat helemaal geen woord is maar bijvoorbeeld punctuatie.\n",
    "\n",
    "We zijn aan het werken aan een classificatie systeem op basis van de gebruikte woorden in de tekst. Nu geeft spaCy standaard een `Document` terug (een lijst van `Token`s). Omdat we vooral geïnteresseerd zijn in de frequentie van woorden in een tekst is dit een onhandig formaat. Elke keer als je wilt weten hoe vaak een woord in een document voorkomt zou je het het hele document moeten doorzoeken. Dus kunnen we beter een andere structuur gebruiken: een _bag of words_.\n",
    "\n",
    "Een _bag of words_ is niks meer dan een datastructuur die per woord bijhoudt hoe vaak deze voorkomt in een tekst. Het ligt voor de hand om hier `Series` voor te gebruiken in Pandas. Een `Series` met als index de woorden en als waardes de hoeveelheid keer dat een woord voorkomt in de tekst. Zo kan je straks bijvoorbeeld heel snel [uitvinden](https://en.wikipedia.org/wiki/List_of_films_that_most_frequently_use_the_word_%22fuck%22) welke films sowieso een \"PG-13 rating\" of \"R-rating\" hebben.\n",
    "\n",
    "### Vraag 1\n",
    "\n",
    "\\[1 pt.\\]\n",
    "\n",
    "Implementeer de functie `bag_of_words` hieronder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb6eb38a204f9b37da9dc32107e48392",
     "grade": false,
     "grade_id": "cell-c8a169592338caa9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def lemmas(doc):\n",
    "    \"\"\"\n",
    "    Returns a list of lemmas \n",
    "    \"\"\"\n",
    "    return [word.lemma_ for word in doc if word.is_alpha and not (word.is_punct or word.is_space or word.is_stop)]\n",
    "\n",
    "def bag_of_words(doc):\n",
    "    \"\"\"\n",
    "    Takes a spaCy doc and returns a bag of words.\n",
    "    Does not include any punctuation, whitespace or stop words in the resulting bag.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    \n",
    "    \n",
    "# Bag of words for The Conjuring\n",
    "print(bag_of_words(movie_subs.loc[103688]).sort_values(ascending = False))\n",
    "\n",
    "# Bag of words for The Lawnmower Man\n",
    "print(bag_of_words(movie_subs.loc[1037]).sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e761e6af6010381ca7e0b3897c787fa6",
     "grade": true,
     "grade_id": "test_7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "answers.test_7(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f41f154d2b7a0dc17ebe1d1915786865",
     "grade": false,
     "grade_id": "cell-662c7f1b6eb997dd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Tijdbesparing\n",
    "\n",
    "Het berekenen van de bag of words voor alle 7149 films waarvoor we de ondertitels hebben kost erg veel tijd. Opdat je jouw computer niet uren hoeft te laten rekenen, hebben we alle bag of words voor alle subtitles gegenereerd en meegeleverd. Onderstaande stukje code laadt deze in en slaat ze op in een `Series` genaamd bags. Dit kan een paar minuten duren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a6aadf3017f599099671001527b9ea8",
     "grade": false,
     "grade_id": "cell-4cb25ff654aa7754",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "bag_path = \"./data/bags\"\n",
    "\n",
    "# Find all bag of words files\n",
    "filenames = os.listdir(bag_path)\n",
    "\n",
    "movie_ids = []\n",
    "bags = []\n",
    "\n",
    "# Tokenize ieder van de losse ondertitelfiles\n",
    "for ix, filename in enumerate(tqdm(filenames)):\n",
    "    path = f'{bag_path}/{filename}'\n",
    "    movie_ids.append(int(filename.split(\".\")[0]))\n",
    "    # Lees, verwerk, sla op...\n",
    "    bag = pd.read_pickle(path)\n",
    "    bags.append(bag)\n",
    "        \n",
    "# ... en maak de series\n",
    "bags = pd.Series(index=movie_ids, data=bags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8bd2ce95422f1a9b60ece203a485c48",
     "grade": false,
     "grade_id": "cell-75e90556cbf395a0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Term Frequency\n",
    "Welke termen (woorden) komen er het meeste voor in een film? Dit kunnen we nu halen uit de bag of words. Dit is bijvoorbeeld de top 20 voor de film Die Hard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "947d7424955e6a3bdc8f913b8048eb55",
     "grade": false,
     "grade_id": "cell-c8aaaee2cd19ec7b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "die_hard = bags[1036]\n",
    "die_hard_sorted = die_hard.sort_values(ascending=False)\n",
    "display(die_hard_sorted[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b01dc797f652c19f050f4e30ac930e79",
     "grade": false,
     "grade_id": "cell-2169c6bd8e75b9fe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Als we willen weten hoe representatief een term is voor een bepaalde film zegt alléén het aantal aantal voorkomens nog niet zo veel. Dit hangt onder andere af van de lengte van de film of de hoeveelheid aan dialoog. Laten we daarom kijken naar hoe vaak een term is gebruikt ten opzichte van het totaal aantal termen. Dit noemen we de _term frequency_\n",
    "\n",
    "Laten we de ondertiteling file van Die Hard even document $d$ noemen. In dit document komt de term ($t$) John 37 keer voor. En het document bevat in totaal 3549 woorden. In dat geval kunnen we de term frequency $t$ in document $d$ als volgt uitrekenen:\n",
    "\n",
    "$$\n",
    "\\textrm{tf}(t,d) = \\frac{37}{3549} = 0.0104\n",
    "$$\n",
    "\n",
    "De formele definitie voor de term frequency $\\textrm{tf}(t, d)$ van term $t$ in document $d$:\n",
    "$$\n",
    "\\textrm{tf}(t, d) = \\frac{\\textrm{f}_{t, d}}{\\sum_{t'\\in d} \\textrm{f}_{t',d}}\n",
    "$$\n",
    "\n",
    "Waar $\\textrm{f}_{t, d}$ het aantal keer is dat term $t$ in document $d$ voorkomt.\n",
    "\n",
    "### Vraag 2\n",
    "\n",
    "\\[1 pt.\\]\n",
    "\n",
    "Implementeer de functie `term_frequency` hieronder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f652652712ff861741c05a5bf48c9c1",
     "grade": false,
     "grade_id": "cell-8b532c9192372b31",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def term_frequency(bag):\n",
    "    \"\"\"Takes in a bag of words, returns the term frequency of every term in bag as a Series.\"\"\"\n",
    "    # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "885b5f46246e774751647ae6de4ba1d8",
     "grade": true,
     "grade_id": "test_8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "answers.test_8(doc, bag_of_words, term_frequency, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67d5e1a766edd7af21d4e4f7ec59af1e",
     "grade": false,
     "grade_id": "cell-2d4433ee173e6aab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "Alleen de TF van eeen woord voor een document zegt ook nog niet zo veel. Woorden die over het algemeen vaak voorkomen, zoals `come` en `go`, zeggen niet zo heel veel over de film Die Hard, maar ze hebben wel een hoge TF. Om deze eruit te filteren kunnen we gebruik maken van de Inverse Document Frequency (IDF). De IDF is een waarde voor een woord die laag is als het woord in veel documenten voorkomt. Hierbij kijken we naar alle documenten in de dataset.\n",
    "\n",
    "Met andere woorden de IDF is een maat die aangeeft hoe _onderscheidend_ een term is gegeven een verzameling van documenten (films in ons geval). Hoeveel documenten zijn er? En in hoeveel daarvan komt de term in kwestie voor? \n",
    "\n",
    "Stel dat de volledige verzameling $D$ uit 7154 documenten bestaat en de term $t$ John in 1897 van die documenten voorkomt, dan is de IDF:\n",
    "\n",
    "$$\n",
    "\\textrm{idf}(t, D) = \\log(\\frac{7154}{1897}) \\approx 1.327\n",
    "$$\n",
    "\n",
    "De formele definitie is:\n",
    "\n",
    "$$\n",
    "\\textrm{idf}(t, D) = \\log(\\frac{|D|}{|D_t|})\n",
    "$$\n",
    "\n",
    "Waarbij $|D|$ het totaal aantal documenten is en $|D_t|$ het aantal documenten met term $t$ erin.\n",
    "\n",
    "N.B. In deze formule staat een _log_. Er zijn een aantal goede redenen voor om hier een log te gebruiken, maar die gaan voorbij de scope van deze opdracht. Als je hier toch graag meer over wilt weten, dan kan je je storten op [dit artikel](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.438.2284&rep=rep1&type=pdf). Dit is een vrij technisch artikel en je hebt niet alle wiskunde en statistiek gehad om alles te kunnen volgen, maar het kan toch leerzaam zijn het eens door te lezen.\n",
    "\n",
    "### Vraag 3\n",
    "\n",
    "\\[1 pt.\\]\n",
    "\n",
    "Implementeer nu de functie `inverse_document_frequency` hieronder.\n",
    "\n",
    "**Let op** Afhankelijk van hoe je `inverse_document_frequency` implementeert kan het best lang duren om de IDF voor alle termen uit te rekenen. Je kan daarom beter debuggen met een subset van alle bags, zeg 10 of 100 bags. Ook hebben wij de IDF voor alle termen al uitgerekend en meegeleverd, deze wordt in een latere cel ingeladen. Je hoeft dus de IDF voor alle termen niet zelf te berekenen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c2467dba75668b75a9e4766980c6539",
     "grade": false,
     "grade_id": "cell-c24de69c54225bd6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "def inverse_document_frequency(bags):\n",
    "    \"\"\"Takes in a Series of bag of words, returns the IDF for every term in the bags in a Series.\"\"\"\n",
    "    # TODO\n",
    "    \n",
    "    \n",
    "    \n",
    "idf = inverse_document_frequency(bags[120:130])\n",
    "print(idf.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c62aa7e937ae12255dad19a8df61069",
     "grade": true,
     "grade_id": "test_9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "answers.test_9(nlp, bag_of_words, inverse_document_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adfca0371fb26f63180de5f1204fb1f2",
     "grade": false,
     "grade_id": "cell-676d6173a2074060",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Tijdbesparing\n",
    "We hebben de IDF voor alle termen al voor je uitgerekend. Zo hoef je niet op je computer te wachten. Onderstaande stukje code laadt de IDF Series in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a70a8253710cd63c8f6a88b11216eccf",
     "grade": false,
     "grade_id": "cell-145d5a93d1014f8c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "idf = pd.read_pickle('./data/idf.pkl')\n",
    "display(idf.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c978101d81df497ea843554500e1395",
     "grade": false,
     "grade_id": "cell-e7f737b3bca32ca4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TF-IDF\n",
    "Term frequency en inverse document frequency kunnen we combineren. Zo krijgen we een score voor hoe relevant een term is voor een document (film). \n",
    "\n",
    "In het voorbeeld dat we hierboven hebben gebruikt is de TF voor John ($t$) in het subtitle document ($d$) Die Hard ongeveer 0.0104 en de IDF voor het woord John in onze corpus ($D$) ongeveer 1.327. De TF-IDF score is dan simpelweg de twee vermenigvuldigd:\n",
    "\n",
    "$$\n",
    "\\textrm{tf-idf}(t, d, D) \\approx 0.0104 * 1.327 \\approx 0.0138\n",
    "$$\n",
    "\n",
    "De formele definitie:\n",
    "\n",
    "$$\n",
    "\\textrm{tf-idf}(t, d, D) = \\textrm{tf}(t, d) * \\textrm{idf}(t, D)\n",
    "$$\n",
    "\n",
    "### Vraag 4\n",
    "\n",
    "\\[1 pt.\\]\n",
    "\n",
    "Implementeer `tf_idf` hieronder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ea3ded9f46eb872fc486fb11fa571de",
     "grade": false,
     "grade_id": "cell-b492953da5d1320f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tf_idf(bag, idf):\n",
    "    \"\"\"\n",
    "    Takes in a bag of words and a Series containing IDF scores, \n",
    "    returns tf_idf score for every word in bag as a Series\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    \n",
    "    \n",
    "tf_idf(bags[1036], idf).sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "505af0d39d88fc53ef186593d57a8866",
     "grade": true,
     "grade_id": "test_11",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "answers.test_11(nlp, bag_of_words, tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bcf6fd46671009be6df8dc98f43e0d69",
     "grade": false,
     "grade_id": "cell-258d8c59f7ef80fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Vraag 5\n",
    "\n",
    "\\[2 pt.\\]\n",
    "\n",
    "Kijk naar de resultaten van `tf_idf`. Je ziet een lijst met woorden die duidelijk karakteristiek zijn voor de film Die Hard. Het zou alleen kunnen dat er woorden tussen zitten die niet nuttig zijn voor het vergelijken met andere films. Bijvoorbeeld \"McClane\" is het hoofpersonage van Die Hard. Die naam is heel karakterisitiek voor de film, maar het is geen nuttig woord als we willen weten of Die Hard op een andere film lijkt. Waarom niet? En zou je een manier kunnen verzinnen om dat soort woorden te vermijden? (Je hoeft dit niet te implementeren!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62acf6482f32426e5963ceac18f39015",
     "grade": true,
     "grade_id": "cell-5b8a10654935540c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b889585393da033e68474849e800c643",
     "grade": false,
     "grade_id": "cell-567e6f23a73e6c2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Vraag 6\n",
    "\n",
    "\\[1 pt.\\]\n",
    "\n",
    "Het bijhouden van een volledige `bag_of_words` gaat ten koste van de snelheid van veel algoritmen. In een praktijk, met geavanceerde hardware, hoeft dat geen probleem te zijn, maar het kan lonen om de `bag_of_words` kleiner te maken. Gelukkig zijn we in de praktijk vooral geinterreseerd in bepaalde delen van de `bag_of_words`. Hoe zouden we de `bag_of_words` kunnen reduceren met zoveel mogelijk behoud van effectiviteit? (Er is hier niet een enkel goed antwoord, het gaat om de redenatie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32991faa27c4226efd238a2b87ec790f",
     "grade": true,
     "grade_id": "cell-25129bec11ca44d2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "\n",
    "Je hebt nu een manier om films te vectoriseren. De output van de `tf_idf`-functie _is_ de vectorisatie van een film. Hiermee kan je een utility matrix maken. Het kan trouwens hiervoor handig zijn om niet de `tf-idf` score te gebruiken van _alle_ woorden, maar bijvoorbeeld alleen de top-N van woorden met de laagste IDF score.\n",
    "\n",
    "Je krijgt dan een utility matrix (vectorisatie) die er ongeveer zo uitziet:\n",
    "\n",
    "<img src = \"./include/tf-idf-vectorization.png\" width = 70%>\n",
    "\n",
    "De features zijn de woorden en de waardes zijn de tf-idf scores voor de betreffend film.\n",
    "\n",
    "Als je de matrix beperkt tot de top-N woorden met de laagste IDF score, is het handig om eerst de stopwoorden er uit te filteren. Stopwoorden zijn woorden die heel veel voorkomen (en dus een hele lage IDF score hebben), maar weinig informatie toevoegen (woorden zoals \"so\", \"why\", \"yes\", ...).\n",
    "\n",
    "spaCy heeft een ingebouwde lijst met stopwoorden genaamd `STOP_WORDS`. Die kan je zo gebruiken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b05dedf5c56de51f7de93f36adea218",
     "grade": false,
     "grade_id": "cell-d13b00212cb6d2ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "if 'why' in STOP_WORDS:\n",
    "    print('\"why\" is a stop word')\n",
    "else:\n",
    "    print('\"why\" is not a stop word')\n",
    "    \n",
    "if 'supercalifragilisticexpialidocious' in STOP_WORDS:\n",
    "    print('\"supercalifragilisticexpialidocious\" is a stop word')\n",
    "else:\n",
    "    print('\"supercalifragilisticexpialidocious\" is not a stop word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a4e14e828a492390fa0e4552a494f94",
     "grade": false,
     "grade_id": "cell-8c792c47a00b21b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Vraag 7\n",
    "\n",
    "\\[2 pt.\\]\n",
    "\n",
    "Schrijf hieronder de functie `tf_idf_vectorizer`. Deze functie heeft als input:\n",
    "\n",
    "- `bags`, de bag of words van alle films\n",
    "- `idf`, de idf scores van alle woorden voor de hele dataset\n",
    "- `max_features`, het maximale aantal woorden om te gebruiken als features (top-N met laagste idf score)\n",
    "- `stopwords`, woorden die je mag negeren (deze tellen dus ook niet mee voor de max-features)\n",
    "\n",
    "De ouput is een dataframe met de films als rijen, de woorden als kolommen en de tf-idf scores als waardes. Zie ook het plaatje hierboven.\n",
    "\n",
    "- De `idf` input geeft een reeks van woorden met IDF waardes. De eerste stap is een subselectie maken door alle `stopwords` uit de lijst te halen en vervolgens de top-N woorden met laagste idf score te selecteren. Hoeveel woorden de top-N bevat wordt gegeven door `max_features`. Implementeer en test dit deel eerst. \n",
    "\n",
    "- Je moet vorvolgens in de functie voor elke film en voor elk woord de tf_idf score berkenen. Je kan hier natuurlijk de `tf_idf`-functie gebruiken die je bij vraag 4 hebt geïmplementeerd.\n",
    "\n",
    "De functie wordt hieronder aangeroepen op slechts een kleine selectie van films. De tf-idf vectorisatie bereken voor de hele dataset zou onpraktisch veel tijd kosten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7b416b53bb83e9a62a722296a0d23e9",
     "grade": false,
     "grade_id": "cell-4b93c42c5199036f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tf_idf_vectorizer(bags, idf, max_features = 500, stopwords = set()):\n",
    "    # TODO\n",
    "    \n",
    "\n",
    "# load set with stop words from spaCy\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "# selection of movies to vectorize (the entire dataset will take way too much time)\n",
    "selection = [1032, 1036, 103253, 103335, 103341, 106487, 106782, 109864, 110, 110102, 1103, 117529, 1222, 122882]\n",
    "\n",
    "# apply function\n",
    "tf_idf_vectorization = tf_idf_vectorizer(bags.loc[selection], idf, max_features = 1000, stopwords = STOP_WORDS)\n",
    "display(tf_idf_vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52a610facd6bbbc251ff64c6974eb6bb",
     "grade": false,
     "grade_id": "cell-49345c823c61a4aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run de code hieronder om te zien welke films je zojuist hebt gevectoriseert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a53a39fa1a6eaa60ba0ba5e35589c6bb",
     "grade": false,
     "grade_id": "cell-4819fca4af3655e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "datapath = \"./data/ml-latest-small\"\n",
    "df_movies = pd.read_csv(f\"{datapath}/movies.csv\", index_col=0)\n",
    "display(df_movies.loc[tf_idf_vectorization.index, 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3c54454b82be20fbc0b2c0d57f27634",
     "grade": false,
     "grade_id": "cell-d5854639bae9d855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test je uitwerking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b5963ce54d528a29af1eb77bc88fb80",
     "grade": true,
     "grade_id": "cell-9b90f4299b71f23e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answers.test_12(tf_idf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82a53c390979a16d1604603ace689f87",
     "grade": false,
     "grade_id": "cell-ebb76369d1be2666",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Is dit een goede uitkomst? Dat is zo op het oog natuurlijk niet meteen te zeggen. We kunnen wel even als sanity check kijken wat het woord is met de hoogste tf-idf score voor elke film en kijken of dat logisch lijkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f92545162683730fbc684ab032d5d100",
     "grade": false,
     "grade_id": "cell-2e83591c3b9238e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# link vectorization to movie titles\n",
    "tf_idf_vectorization_with_titles = tf_idf_vectorization.copy()\n",
    "tf_idf_vectorization_with_titles.index = df_movies.loc[tf_idf_vectorization.index, 'title']\n",
    "\n",
    "# highest scoring feature\n",
    "best_feature = tf_idf_vectorization_with_titles.idxmax(axis=1)\n",
    "print(best_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1811da8057eb7aa2e718657efe702bdd",
     "grade": false,
     "grade_id": "cell-69492e736a02de8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Vraag 8\n",
    "\\[2 pt.\\]\n",
    "\n",
    "Maak hieronder een cosine similarity matrix voor deze vectorisatie. Je kan hiervoor de functie `create_similarity_matrix_cosine` gebruiken. Sla het resultaat op in de variabele `similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d83b1707e8fed6d416ec0c48897e4ee",
     "grade": false,
     "grade_id": "cell-ea0d6a45881cbd4c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from cf1 import create_similarity_matrix_cosine\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "300698c479a4a916885cfa4d3c2e5f7c",
     "grade": false,
     "grade_id": "cell-7e16875a9d4460fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test je uitwerking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c6950694e0854a5821be04049b7eaf5",
     "grade": true,
     "grade_id": "cell-c862b84a25a7d107",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answers.test_14(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96a6a70f5475635b4f534a64600ad5bf",
     "grade": false,
     "grade_id": "cell-ab9f00f074fd6cd1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We hebben te weinig films in onze similarity matrix zitten om op een zinnige manier ratings te kunnen voorspellen en aanbevelingen te kunnen doen. Je zou in principe de utility en similarity matrix voor de hele data set kunnen bepalen en dan kan je voorspellingen doen zoals je dat ook in de vorige modules hebt gedaan. \n",
    "\n",
    "Je kan alleen de prestaties (mse, precision, recall, etc.) van het algorithme niet direct vergelijken met die uit de vorige opdrachten. De dataset bevat niet de ondertitels voor alle films uit `ratings.csv`.\n",
    "\n",
    "### Vraag 9\n",
    "\\[2 pt.\\]\n",
    "\n",
    "a) Waarom kan je dan de prestaties niet direct vergelijken?\n",
    "\n",
    "b) Wat zou je moeten doen om ervoor te zorgen dat je de prestaties van dit algoritme wel goed kan vergelijken met die van eerdere algoritmes (gegeven dat je niet _meer_ ondertitelingen kan vinden)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16d514cb5119e6a608da6dfbe8e3eb98",
     "grade": true,
     "grade_id": "cell-cfda6682121c08fe",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6c9c2d204bcc2b78aeb259a94965b91",
     "grade": false,
     "grade_id": "cell-cbf6b36dadb4cf84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "TF_IDF vectorisatie is slechts een manier om natuurlijke taal data te vectoriseren. SpaCy zelf heeft ook een ingebouwde aanpak hiervoor. Deze maakt gebruik van word2vec. Een vectorisatie die gebaseerd is op neurale netwerken. Hier ga je in de volgende module naar kijken."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
